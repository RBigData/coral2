<h1 id="big-data-benchmarks">Big Data Benchmarks</h1>
<p>This is a set of three &quot;big data&quot; benchmarks. The benchmarks utilize three very common statistics/machine learning techniques, namely principal components analysis (PCA), k-means clustering, and support vector machine (SVM). This gives one dimension reduction technique, one unsupervised technique, and one supervised technique.</p>
<p>For each benchmark, the data must be distributed across ranks by row, and is assumed to be taller than it is wide. Each uses MPI for communication.</p>
<p>Of the three benchmarks, all are available as parallel R codes, and one (PCA) is additionally available as a standalone C++ code. For each of the R benchmarks, you will need to install install the kazaam package and all of its dependencies which are available in the <code>source/</code> tree. To install the C++ code (for the PCA benchmark), it should be enough to just run <code>make</code>. See <code>source/README</code> for details.</p>
<h2 id="running-the-bencharks">Running the Bencharks</h2>
<p>Each of the benchmarks (R and C++ alike) accepts a number of <em>local</em> rows and <em>global</em> columns. The total number of rows grows proportionally to the number of MPI ranks. Thus, each benchmark measures scaling in the weak sense.</p>
<p>Run the C++ code from the project root via:</p>
<pre><code>mpirun -np num_ranks ./source/cxx/build/princomp num_local_rows num_global_cols</code></pre>
<p>Run the R code via:</p>
<pre><code>mpirun -np num_ranks Rscript princomp.r num_local_rows num_global_cols</code></pre>
<p>For example, to run the MPI code with 16 ranks, 50 total columns, and a total of 16,000 rows (local number of rows 1000), you would run:</p>
<pre><code>mpirun -np 16 princomp 1000 50</code></pre>
<p>For example, to generate 1 GB local sizes with 100 columns, you would need to specify 1250000 local rows. For a 1 GiB local size with 100 columns, you would need 1342177 local rows.</p>
<h2 id="benchmark-details">Benchmark Details</h2>
<p>Below we describe the benchmarks in detail.</p>
<h3 id="pca">PCA</h3>
<p>The benchmark is defined as computing the first and last of the &quot;standard deviations&quot; from a PCA on a distributed matrix by way of taking the square roots of the eigenvalues of the covariance matrix. The &quot;first and last&quot; requirement is to avoid approximate methods. Using the covariance matrix is mathematically equivalent to computing the SVD of the mean-centered input matrix, although it is computationally easier. The data should be random normal.</p>
<p>Unlike the other benchmarks, the PCA benchmark is available in both R and C++. Our implementation of the R benchmark can be found at <code>benchmarks/r/kmeans.r</code>. The C++ benchmark is self-contained; simply run the executable generated from running <code>make</code> in <code>source/cxx/</code>.</p>
<h3 id="k-means">k-means</h3>
<p>The benchmark is defined by computing the observation labels (class assignments) and centroids for k=2, 3, and 4. The data should consist of rows sampled from one of 3 random normal distributions: one with mean 0, one with mean 2, and one with mean 10. Each should have variance 1. The rows should be drawn at random from these distributions.</p>
<p>Our implementation of the benchmark can be found at <code>benchmarks/r/kmeans.r</code>.</p>
<h3 id="svm">SVM</h3>
<p>The benchmark consists of a linear 2-class SVM fit, calculating the feature weights. The data matrix of predictors should be random normal. The response should be taken to be 1 if the first predictor is greater than 0, and -1 otherwise.</p>
<p>Our implementation of the benchmark can be found at <code>benchmarks/r/svm.r</code>.</p>
<h2 id="validation">Validation</h2>
<p>We use random data in the benchmarks in order to keep the benchmarks as amenable to every hardware solution possible. We strongly believe this approach is to the advantage of every vendor. However, this makes the benchmark runs difficult to verify. So we have included small verification scripts to be run in companion with the benchmarks themselves.</p>
<p>It would be possible to do something underhanded (fast but incorrect) on the large benchmarks while passing the small scale tests. But remember that we have chosen this path for the vendor's benefit. Please do not betray that trust.</p>
<p>Each validation script should be run on two nodes and use the same (specified) kernel as its benchmark counterpart. Each will use the famous iris dataset of R. A. Fisher (included). The rows of the dataset have been randomly shuffled and the species variable has been coded to 1=setosa, 2=versicolor, and 3=virginica. Any other settings we leave to the vendor. Performance measurements are not desired, only the validation.</p>
<h3 id="svd">SVD</h3>
<p>This validation script shows that the PCA benchmark is working correctly by testing the SVD kernel. The validation consists of reading the iris dataset, removing the species column, factoring the resulting matrix, and then multiplying the factored matrices (from SVD) back together. The mean absolute error (average of the difference in absolute value) of the two matrices should be computed. The test passes if this value is less than the square root of machine epsilon for each type (as specified by IEEE 754).</p>
<h3 id="k-means-1">k-means</h3>
<p>Using k=3 centroids (the true value), and 100 starts using seeds 1 to 100, the labels for each observation should be computed. These will be compared against the true values (from the 'species' label of the dataset) using <a href="https://en.wikipedia.org/wiki/Rand_index">rand measure</a>. Take the largest among these values. This should be greater than 75% to be considered successful.</p>
<h3 id="svm-1">SVM</h3>
